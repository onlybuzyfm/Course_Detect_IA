{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jose\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solo una vez al inicio\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()  # No entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        return math.exp(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(text):\n",
    "    probs = [freq / len(text) for freq in Counter(text).values()]\n",
    "    return -sum(p * math.log2(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ngram_metrics(text, n=3):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "    ngrams = vectorizer.fit_transform([text])\n",
    "    return dict(zip(vectorizer.get_feature_names_out(), ngrams.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def burstiness(text):\n",
    "    words = text.split()\n",
    "    word_counts = Counter(words)\n",
    "    freqs = list(word_counts.values())\n",
    "    if len(freqs) < 2:\n",
    "        return 0.0\n",
    "    return np.std(freqs) / np.mean(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams(text, n=3, top_k=10):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "    ngrams = vectorizer.fit_transform([text])\n",
    "    freqs = ngrams.toarray()[0]\n",
    "    ngram_names = vectorizer.get_feature_names_out()\n",
    "    sorted_ngrams = sorted(zip(ngram_names, freqs), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_ngrams[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(zip_path, extract_to=\"pdfs_temp\"):\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(extract_to):\n",
    "        for f in files:\n",
    "            if f.endswith(\".pdf\"):\n",
    "                pdf_files.append(os.path.join(root, f))\n",
    "    \n",
    "    return pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_in_zip(zip_path):\n",
    "    pdf_paths = extract_zip(zip_path)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for path in pdf_paths:\n",
    "        print(f\"Procesando: {os.path.basename(path)}\")\n",
    "        raw = extract_text_from_pdf(path)\n",
    "        clean = preprocess_text(raw)\n",
    "        ent = entropy(clean)\n",
    "        bur = burstiness(clean)\n",
    "        ppl = calculate_perplexity(clean)\n",
    "\n",
    "        results.append({\n",
    "            \"archivo\": os.path.basename(path),\n",
    "            \"entropia\": round(ent, 4),\n",
    "            \"burstiness\": round(bur, 4),\n",
    "            \"perplejidad\": round(ppl, 4)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(zip_path, output_csv=\"resultados.csv\"):\n",
    "    df = process_pdfs_in_zip(zip_path)\n",
    "    \n",
    "    shutil.rmtree(\"pdfs_temp\", ignore_errors=True)\n",
    "    print(\"ðŸ§¹ Carpeta 'pdfs_temp' eliminada.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: D1.pdf\n",
      "Procesando: D2.pdf\n",
      "Procesando: D3.pdf\n",
      "Procesando: D4.pdf\n",
      "Procesando: D5.pdf\n",
      "Procesando: D6.pdf\n",
      "ðŸ§¹ Carpeta 'pdfs_temp' eliminada.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>archivo</th>\n",
       "      <th>entropia</th>\n",
       "      <th>burstiness</th>\n",
       "      <th>perplejidad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1.pdf</td>\n",
       "      <td>4.0744</td>\n",
       "      <td>0.5679</td>\n",
       "      <td>160.4222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2.pdf</td>\n",
       "      <td>4.0528</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>108.0962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D3.pdf</td>\n",
       "      <td>3.9881</td>\n",
       "      <td>0.6576</td>\n",
       "      <td>219.6888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D4.pdf</td>\n",
       "      <td>4.0383</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>110.8801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5.pdf</td>\n",
       "      <td>4.1078</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>173.1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D6.pdf</td>\n",
       "      <td>4.0451</td>\n",
       "      <td>0.4651</td>\n",
       "      <td>196.9493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  archivo  entropia  burstiness  perplejidad\n",
       "0  D1.pdf    4.0744      0.5679     160.4222\n",
       "1  D2.pdf    4.0528      0.5415     108.0962\n",
       "2  D3.pdf    3.9881      0.6576     219.6888\n",
       "3  D4.pdf    4.0383      0.6880     110.8801\n",
       "4  D5.pdf    4.1078      0.7635     173.1369\n",
       "5  D6.pdf    4.0451      0.4651     196.9493"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_resultados = run_pipeline(\"Documents.zip\")\n",
    "\n",
    "display(df_resultados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_umbral_penalizacion(df, sensibilidad=1.0):  # sensibilidad = IQR factor\n",
    "    umbrales = {}\n",
    "\n",
    "    for columna in [\"entropia\", \"burstiness\", \"perplejidad\"]:\n",
    "        q1 = df[columna].quantile(0.25)\n",
    "        q3 = df[columna].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        lim_inf = q1 - sensibilidad * iqr\n",
    "        lim_sup = q3 + sensibilidad * iqr\n",
    "\n",
    "        umbrales[columna] = {\n",
    "            \"q1\": q1,\n",
    "            \"q3\": q3,\n",
    "            \"inferior\": lim_inf,\n",
    "            \"superior\": lim_sup\n",
    "        }\n",
    "\n",
    "    return umbrales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marcar_sospechosos_compuesto(df, ppl_ratio=0.6, entropia_min=3.5):\n",
    "    max_ppl = df[\"perplejidad\"].max()\n",
    "    df[\"perplejidad_relativa\"] = df[\"perplejidad\"] / max_ppl\n",
    "\n",
    "    def sospechoso(row):\n",
    "        condiciones = [\n",
    "            row[\"perplejidad_relativa\"] < ppl_ratio,\n",
    "            row[\"entropia\"] < entropia_min\n",
    "        ]\n",
    "        return \"SÃ­\" if sum(condiciones) >= 1 else \"No\"\n",
    "\n",
    "    df[\"sospechoso\"] = df.apply(sospechoso, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: D1.pdf\n",
      "Procesando: D2.pdf\n",
      "Procesando: D3.pdf\n",
      "Procesando: D4.pdf\n",
      "Procesando: D5.pdf\n",
      "Procesando: D6.pdf\n",
      "ðŸ§¹ Carpeta 'pdfs_temp' eliminada.\n"
     ]
    }
   ],
   "source": [
    "df = run_pipeline(\"Documents.zip\")\n",
    "umbrales = calcular_umbral_penalizacion(df)\n",
    "df_1 = marcar_sospechosos_compuesto(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>archivo</th>\n",
       "      <th>entropia</th>\n",
       "      <th>burstiness</th>\n",
       "      <th>perplejidad</th>\n",
       "      <th>perplejidad_relativa</th>\n",
       "      <th>sospechoso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1.pdf</td>\n",
       "      <td>4.0744</td>\n",
       "      <td>0.5679</td>\n",
       "      <td>160.4222</td>\n",
       "      <td>0.730225</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2.pdf</td>\n",
       "      <td>4.0528</td>\n",
       "      <td>0.5415</td>\n",
       "      <td>108.0962</td>\n",
       "      <td>0.492042</td>\n",
       "      <td>SÃ­</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D3.pdf</td>\n",
       "      <td>3.9881</td>\n",
       "      <td>0.6576</td>\n",
       "      <td>219.6888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D4.pdf</td>\n",
       "      <td>4.0383</td>\n",
       "      <td>0.6880</td>\n",
       "      <td>110.8801</td>\n",
       "      <td>0.504714</td>\n",
       "      <td>SÃ­</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D5.pdf</td>\n",
       "      <td>4.1078</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>173.1369</td>\n",
       "      <td>0.788101</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D6.pdf</td>\n",
       "      <td>4.0451</td>\n",
       "      <td>0.4651</td>\n",
       "      <td>196.9493</td>\n",
       "      <td>0.896492</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  archivo  entropia  burstiness  perplejidad  perplejidad_relativa sospechoso\n",
       "0  D1.pdf    4.0744      0.5679     160.4222              0.730225         No\n",
       "1  D2.pdf    4.0528      0.5415     108.0962              0.492042         SÃ­\n",
       "2  D3.pdf    3.9881      0.6576     219.6888              1.000000         No\n",
       "3  D4.pdf    4.0383      0.6880     110.8801              0.504714         SÃ­\n",
       "4  D5.pdf    4.1078      0.7635     173.1369              0.788101         No\n",
       "5  D6.pdf    4.0451      0.4651     196.9493              0.896492         No"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
